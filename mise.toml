# =============================================================================
# Configuration Mise pour Analyse PySpark
# =============================================================================
# Ce fichier contient toutes les tÃ¢ches nÃ©cessaires pour le projet PySpark
# incluant la gestion du cluster Spark (master/workers), l'analyse de donnÃ©es,
# les tests, et les utilitaires de dÃ©veloppement.
#
# Usage: mise run <task>
# Liste des tÃ¢ches: mise tasks
# =============================================================================

[env]
# Configuration Java pour PySpark
JAVA_HOME = "/opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home"
# Configuration Spark
# Note: SPARK_HOME est optionnel si Spark n'est pas installÃ© sÃ©parÃ©ment
# PySpark intÃ©grÃ© fonctionnera en mode local sans SPARK_HOME
# SPARK_HOME = "{{ env.HOME }}/.local/share/spark"
SPARK_MASTER_HOST = "localhost"
SPARK_MASTER_PORT = "7077"
SPARK_MASTER_WEBUI_PORT = "8080"
SPARK_WORKER_WEBUI_PORT = "8081"
SPARK_WORKER_CORES = "2"
SPARK_WORKER_MEMORY = "2g"
# Configuration PySpark
PYSPARK_PYTHON = "python3"
PYSPARK_DRIVER_PYTHON = "python3"
# Dataset
DATA_FILE = "data/employee_data.csv"

[tools]
python = "3.12"

# =============================================================================
# CLUSTER SPARK - MASTER
# =============================================================================

[tasks.spark-master-start]
description = "ğŸš€ DÃ©marrer le Spark Master"
run = """
#!/bin/bash
set -e
echo "ğŸš€ DÃ©marrage du Spark Master..."
echo "   Host: $SPARK_MASTER_HOST"
echo "   Port: $SPARK_MASTER_PORT"
echo "   WebUI: http://localhost:$SPARK_MASTER_WEBUI_PORT"

if command -v spark-class &> /dev/null; then
    spark-class org.apache.spark.deploy.master.Master \
        --host $SPARK_MASTER_HOST \
        --port $SPARK_MASTER_PORT \
        --webui-port $SPARK_MASTER_WEBUI_PORT
else
    echo "âš ï¸  Spark n'est pas installÃ© localement."
    echo "   Utilisation du mode local intÃ©grÃ© Ã  PySpark."
    echo "   Pour un cluster complet, installez Spark: brew install apache-spark"
fi
"""

[tasks.spark-master-stop]
description = "ğŸ›‘ ArrÃªter le Spark Master"
run = """
#!/bin/bash
echo "ğŸ›‘ ArrÃªt du Spark Master..."
pkill -f "spark.*Master" || echo "Aucun Master en cours d'exÃ©cution"
echo "âœ“ Spark Master arrÃªtÃ©"
"""

# =============================================================================
# CLUSTER SPARK - WORKERS
# =============================================================================

[tasks.spark-worker-start]
description = "ğŸ‘· DÃ©marrer un Spark Worker"
run = """
#!/bin/bash
set -e
MASTER_URL="spark://$SPARK_MASTER_HOST:$SPARK_MASTER_PORT"
echo "ğŸ‘· DÃ©marrage du Spark Worker..."
echo "   Master URL: $MASTER_URL"
echo "   Cores: $SPARK_WORKER_CORES"
echo "   Memory: $SPARK_WORKER_MEMORY"
echo "   WebUI: http://localhost:$SPARK_WORKER_WEBUI_PORT"

if command -v spark-class &> /dev/null; then
    spark-class org.apache.spark.deploy.worker.Worker \
        --cores $SPARK_WORKER_CORES \
        --memory $SPARK_WORKER_MEMORY \
        --webui-port $SPARK_WORKER_WEBUI_PORT \
        $MASTER_URL
else
    echo "âš ï¸  Spark n'est pas installÃ© localement."
    echo "   Utilisation du mode local intÃ©grÃ© Ã  PySpark."
fi
"""

[tasks.spark-worker-stop]
description = "ğŸ›‘ ArrÃªter les Spark Workers"
run = """
#!/bin/bash
echo "ğŸ›‘ ArrÃªt des Spark Workers..."
pkill -f "spark.*Worker" || echo "Aucun Worker en cours d'exÃ©cution"
echo "âœ“ Spark Workers arrÃªtÃ©s"
"""

[tasks.spark-workers-start]
description = "ğŸ‘·ğŸ‘· DÃ©marrer plusieurs Spark Workers (2 par dÃ©faut)"
run = """
#!/bin/bash
set -e
NUM_WORKERS=${1:-2}
MASTER_URL="spark://$SPARK_MASTER_HOST:$SPARK_MASTER_PORT"
echo "ğŸ‘· DÃ©marrage de $NUM_WORKERS Spark Workers..."

for i in $(seq 1 $NUM_WORKERS); do
    WORKER_PORT=$((8081 + $i - 1))
    echo "   DÃ©marrage Worker $i sur le port WebUI $WORKER_PORT..."
    
    if command -v spark-class &> /dev/null; then
        spark-class org.apache.spark.deploy.worker.Worker \
            --cores $SPARK_WORKER_CORES \
            --memory $SPARK_WORKER_MEMORY \
            --webui-port $WORKER_PORT \
            $MASTER_URL &
    fi
done

echo "âœ“ $NUM_WORKERS Workers dÃ©marrÃ©s"
"""

# =============================================================================
# CLUSTER SPARK - GESTION COMPLÃˆTE
# =============================================================================

[tasks.spark-cluster-start]
description = "ğŸŒŸ DÃ©marrer le cluster Spark complet (Master + 2 Workers)"
run = """
#!/bin/bash
set -e
echo "ğŸŒŸ DÃ©marrage du cluster Spark complet..."
echo ""

# DÃ©marrer le Master en arriÃ¨re-plan
echo "1ï¸âƒ£  DÃ©marrage du Master..."
if command -v spark-class &> /dev/null; then
    spark-class org.apache.spark.deploy.master.Master \
        --host $SPARK_MASTER_HOST \
        --port $SPARK_MASTER_PORT \
        --webui-port $SPARK_MASTER_WEBUI_PORT &
    sleep 3
    
    # DÃ©marrer les Workers
    echo "2ï¸âƒ£  DÃ©marrage des Workers..."
    for i in 1 2; do
        WORKER_PORT=$((8081 + $i - 1))
        spark-class org.apache.spark.deploy.worker.Worker \
            --cores $SPARK_WORKER_CORES \
            --memory $SPARK_WORKER_MEMORY \
            --webui-port $WORKER_PORT \
            spark://$SPARK_MASTER_HOST:$SPARK_MASTER_PORT &
        sleep 1
    done
    
    echo ""
    echo "âœ… Cluster Spark dÃ©marrÃ©!"
    echo ""
    echo "ğŸ“Š Interfaces Web:"
    echo "   Master UI: http://localhost:$SPARK_MASTER_WEBUI_PORT"
    echo "   Worker 1 UI: http://localhost:8081"
    echo "   Worker 2 UI: http://localhost:8082"
    echo ""
    echo "ğŸ”— URL de connexion pour vos applications:"
    echo "   spark://$SPARK_MASTER_HOST:$SPARK_MASTER_PORT"
else
    echo "âš ï¸  Spark standalone n'est pas installÃ©."
    echo "   Utilisez: brew install apache-spark"
    echo ""
    echo "ğŸ’¡ Alternative: utilisez le mode local intÃ©grÃ© Ã  PySpark"
    echo "   mise run analyse"
fi
"""

[tasks.spark-cluster-stop]
description = "ğŸ›‘ ArrÃªter le cluster Spark complet"
run = """
#!/bin/bash
echo "ğŸ›‘ ArrÃªt du cluster Spark complet..."
pkill -f "spark.*Worker" 2>/dev/null || true
pkill -f "spark.*Master" 2>/dev/null || true
echo "âœ“ Cluster Spark arrÃªtÃ©"
"""

[tasks.spark-cluster-status]
description = "ğŸ“Š Afficher le statut du cluster Spark"
run = """
#!/bin/bash
echo "ğŸ“Š Statut du cluster Spark"
echo "=========================="
echo ""

# VÃ©rifier le Master
if pgrep -f "spark.*Master" > /dev/null; then
    echo "âœ… Master: En cours d'exÃ©cution"
    echo "   URL: spark://$SPARK_MASTER_HOST:$SPARK_MASTER_PORT"
    echo "   WebUI: http://localhost:$SPARK_MASTER_WEBUI_PORT"
else
    echo "âŒ Master: ArrÃªtÃ©"
fi

echo ""

# Compter les Workers
WORKER_COUNT=$(pgrep -f "spark.*Worker" | wc -l | tr -d ' ')
if [ "$WORKER_COUNT" -gt 0 ]; then
    echo "âœ… Workers: $WORKER_COUNT en cours d'exÃ©cution"
else
    echo "âŒ Workers: Aucun en cours d'exÃ©cution"
fi

echo ""
echo "ğŸ’¡ Processus Java Spark:"
jps 2>/dev/null | grep -E "(Master|Worker)" || echo "   Aucun processus Spark trouvÃ©"
"""

# =============================================================================
# ANALYSE DE DONNÃ‰ES
# =============================================================================

[tasks.analyse]
description = "ğŸ“Š ExÃ©cuter l'analyse PySpark principale"
run = """
#!/bin/bash
# DÃ©sactiver SPARK_HOME si Spark n'est pas installÃ© sÃ©parÃ©ment
# pour permettre Ã  PySpark intÃ©grÃ© de fonctionner
if [ ! -d "${SPARK_HOME}/bin" ]; then
    unset SPARK_HOME
fi
uv run python analyse.py
"""

[tasks.analyse-cluster]
description = "ğŸ“Š ExÃ©cuter l'analyse sur le cluster Spark"
run = """
#!/bin/bash
MASTER_URL="spark://$SPARK_MASTER_HOST:$SPARK_MASTER_PORT"
echo "ğŸ“Š ExÃ©cution de l'analyse sur le cluster Spark..."
echo "   Master: $MASTER_URL"
echo ""

if command -v spark-submit &> /dev/null; then
    spark-submit \
        --master $MASTER_URL \
        --deploy-mode client \
        --driver-memory 2g \
        --executor-memory 2g \
        --executor-cores 2 \
        analyse.py
else
    echo "âš ï¸  spark-submit non trouvÃ©, utilisation de PySpark en mode local"
    uv run python analyse.py
fi
"""

[tasks.analyse-local]
description = "ğŸ“Š ExÃ©cuter l'analyse en mode local (sans cluster)"
run = """
#!/bin/bash
echo "ğŸ“Š ExÃ©cution de l'analyse en mode local..."
echo "   Utilisation du mode local[*] de PySpark"
echo ""
uv run python analyse.py
"""

[tasks.notebook]
description = "ğŸ““ Lancer Jupyter Notebook pour l'analyse interactive"
run = """
#!/bin/bash
echo "ğŸ““ Lancement de Jupyter Notebook..."
uv run jupyter notebook analyse_notebook.ipynb
"""

[tasks.lab]
description = "ğŸ§ª Lancer JupyterLab pour l'analyse interactive"
run = """
#!/bin/bash
echo "ğŸ§ª Lancement de JupyterLab..."
uv run jupyter lab
"""

# =============================================================================
# PYSPARK SHELL
# =============================================================================

[tasks.pyspark-shell]
description = "ğŸ Lancer le shell PySpark interactif (mode local)"
run = """
#!/bin/bash
echo "ğŸ Lancement du shell PySpark..."
echo "   Mode: local[*]"
echo ""
uv run pyspark --master local[*]
"""

[tasks.pyspark-shell-cluster]
description = "ğŸ Lancer le shell PySpark connectÃ© au cluster"
run = """
#!/bin/bash
MASTER_URL="spark://$SPARK_MASTER_HOST:$SPARK_MASTER_PORT"
echo "ğŸ Lancement du shell PySpark..."
echo "   Master: $MASTER_URL"
echo ""

if command -v pyspark &> /dev/null; then
    pyspark --master $MASTER_URL
else
    uv run pyspark --master $MASTER_URL
fi
"""

[tasks.spark-sql]
description = "ğŸ’¾ Lancer le shell Spark SQL"
run = """
#!/bin/bash
echo "ğŸ’¾ Lancement du shell Spark SQL..."
if command -v spark-sql &> /dev/null; then
    spark-sql
else
    echo "âš ï¸  spark-sql non disponible, utilisez pyspark Ã  la place"
    echo "   mise run pyspark-shell"
fi
"""

# =============================================================================
# TESTS
# =============================================================================

[tasks.test]
description = "ğŸ§ª ExÃ©cuter les tests unitaires"
run = "uv run pytest test_analyse.py -v"

[tasks.test-cov]
description = "ğŸ§ª ExÃ©cuter les tests avec couverture de code"
run = "uv run pytest test_analyse.py -v --cov=analyse --cov-report=html --cov-report=term"

[tasks.test-watch]
description = "ğŸ§ª ExÃ©cuter les tests en mode watch"
run = "uv run pytest test_analyse.py -v --watch"

# =============================================================================
# DÃ‰VELOPPEMENT
# =============================================================================

[tasks.install]
description = "ğŸ“¦ Installer les dÃ©pendances du projet"
run = """
#!/bin/bash
echo "ğŸ“¦ Installation des dÃ©pendances..."
uv sync
echo "âœ“ DÃ©pendances installÃ©es"
"""

[tasks.install-spark]
description = "ğŸ“¦ Instructions pour installer Apache Spark (macOS)"
run = '''
#!/bin/bash
echo "ğŸ“¦ Installation d'Apache Spark"
echo "=============================="
echo ""
echo "Pour macOS avec Homebrew:"
echo ""
echo "1ï¸âƒ£  Installer Java 17:"
echo "    brew install openjdk@17"
echo ""
echo "2ï¸âƒ£  Installer Apache Spark:"
echo "    brew install apache-spark"
echo ""
echo "3ï¸âƒ£  Configurer les variables d'environnement:"
echo "    export JAVA_HOME=/opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home"
echo "    export SPARK_HOME=/opt/homebrew/opt/apache-spark/libexec"
echo "    export PATH=\$SPARK_HOME/bin:\$PATH"
echo ""
echo "4ï¸âƒ£  VÃ©rifier l'installation:"
echo "    spark-shell --version"
echo ""
echo "ğŸ’¡ Note: PySpark fonctionne aussi sans installer Spark sÃ©parÃ©ment"
echo "   mais le mode cluster nÃ©cessite l'installation complÃ¨te."
'''

[tasks.lint]
description = "ğŸ” VÃ©rifier le code avec ruff"
run = "uv run ruff check ."

[tasks.lint-fix]
description = "ğŸ”§ Corriger les erreurs de lint automatiquement"
run = "uv run ruff check . --fix"

[tasks.format]
description = "âœ¨ Formater le code avec ruff"
run = "uv run ruff format ."

[tasks.check]
description = "ğŸ” VÃ©rifier le typage avec mypy"
run = "uv run mypy analyse.py"

# =============================================================================
# DONNÃ‰ES
# =============================================================================

[tasks.data-info]
description = "ğŸ“‹ Afficher les informations sur le dataset"
run = """
#!/bin/bash
echo "ğŸ“‹ Informations sur le dataset"
echo "=============================="
echo ""
echo "ğŸ“ Fichier: $DATA_FILE"
echo ""

if [ -f "$DATA_FILE" ]; then
    echo "ğŸ“Š Statistiques:"
    echo "   Lignes: $(wc -l < $DATA_FILE)"
    echo "   Colonnes: $(head -1 $DATA_FILE | tr ',' '\n' | wc -l)"
    echo ""
    echo "ğŸ“ En-tÃªtes:"
    head -1 $DATA_FILE | tr ',' '\n' | nl
    echo ""
    echo "ğŸ‘€ AperÃ§u (5 premiÃ¨res lignes):"
    head -6 $DATA_FILE | column -t -s','
else
    echo "âŒ Fichier non trouvÃ©: $DATA_FILE"
fi
"""

[tasks.data-generate]
description = "ğŸ”„ GÃ©nÃ©rer un nouveau dataset de test"
run = """
#!/bin/bash
echo "ğŸ”„ GÃ©nÃ©ration d'un nouveau dataset de test..."
uv run python -c "
import random
import csv

departments = ['IT', 'HR', 'Sales', 'Marketing', 'Finance', 'Operations']
genders = ['M', 'F']

with open('data/employee_data.csv', 'w', newline='') as f:
    writer = csv.writer(f)
    writer.writerow(['id', 'age', 'gender', 'salary', 'department', 'experience_years', 'satisfaction_score', 'performance_rating'])
    
    for i in range(1, 501):
        age = random.randint(22, 60)
        exp = min(age - 22, random.randint(0, 35))
        salary = 30000 + (exp * 2000) + random.randint(-5000, 15000)
        
        writer.writerow([
            i,
            age,
            random.choice(genders),
            salary,
            random.choice(departments),
            exp,
            round(random.uniform(3, 10), 1),
            round(random.uniform(2, 5), 1)
        ])

print('âœ“ Dataset gÃ©nÃ©rÃ©: data/employee_data.csv (500 employÃ©s)')
"
"""

# =============================================================================
# UTILITAIRES
# =============================================================================

[tasks.clean]
description = "ğŸ§¹ Nettoyer les fichiers temporaires"
run = """
#!/bin/bash
echo "ğŸ§¹ Nettoyage des fichiers temporaires..."
rm -rf __pycache__
rm -rf .pytest_cache
rm -rf .mypy_cache
rm -rf .ruff_cache
rm -rf htmlcov
rm -rf .coverage
rm -rf *.egg-info
rm -rf spark-warehouse
rm -rf metastore_db
rm -rf derby.log
find . -name "*.pyc" -delete
find . -name "*.pyo" -delete
find . -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null || true
echo "âœ“ Nettoyage terminÃ©"
"""

[tasks.info]
description = "â„¹ï¸  Afficher les informations du projet"
run = """
#!/bin/bash
echo "â„¹ï¸  Informations du projet"
echo "========================="
echo ""
echo "ğŸ“¦ Projet: analyse-pyspark"
echo "ğŸ Python: $(python --version 2>&1)"
echo "â˜• Java: $(java -version 2>&1 | head -1)"
echo ""
echo "ğŸ“ Structure:"
ls -la
echo ""
echo "ğŸ”§ Variables d'environnement Spark:"
echo "   JAVA_HOME: $JAVA_HOME"
echo "   SPARK_HOME: $SPARK_HOME"
echo "   SPARK_MASTER_HOST: $SPARK_MASTER_HOST"
echo "   SPARK_MASTER_PORT: $SPARK_MASTER_PORT"
"""

[tasks.help]
description = "â“ Afficher l'aide et les commandes disponibles"
run = """
#!/bin/bash
echo "â“ Aide - Analyse PySpark"
echo "========================="
echo ""
echo "ğŸŒŸ CLUSTER SPARK"
echo "   mise run spark-cluster-start    - DÃ©marrer le cluster complet"
echo "   mise run spark-cluster-stop     - ArrÃªter le cluster"
echo "   mise run spark-cluster-status   - Voir le statut du cluster"
echo "   mise run spark-master-start     - DÃ©marrer uniquement le Master"
echo "   mise run spark-worker-start     - DÃ©marrer un Worker"
echo ""
echo "ğŸ“Š ANALYSE"
echo "   mise run analyse                - ExÃ©cuter l'analyse (mode local)"
echo "   mise run analyse-cluster        - ExÃ©cuter sur le cluster"
echo "   mise run notebook               - Ouvrir Jupyter Notebook"
echo "   mise run pyspark-shell          - Shell PySpark interactif"
echo ""
echo "ğŸ§ª TESTS"
echo "   mise run test                   - ExÃ©cuter les tests"
echo "   mise run test-cov               - Tests avec couverture"
echo ""
echo "ğŸ”§ DÃ‰VELOPPEMENT"
echo "   mise run install                - Installer les dÃ©pendances"
echo "   mise run lint                   - VÃ©rifier le code"
echo "   mise run format                 - Formater le code"
echo "   mise run clean                  - Nettoyer les fichiers temp"
echo ""
echo "ğŸ“‹ DONNÃ‰ES"
echo "   mise run data-info              - Info sur le dataset"
echo "   mise run data-generate          - GÃ©nÃ©rer un nouveau dataset"
echo ""
echo "ğŸ’¡ Pour voir toutes les tÃ¢ches: mise tasks"
"""
