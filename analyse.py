"""
Analyse PySpark d'un dataset Kaggle
====================================

Ce script rÃ©alise une analyse complÃ¨te d'un dataset en utilisant PySpark.
Il inclut:
- Chargement des donnÃ©es
- Exploration et statistiques descriptives
- Analyses par groupes
- CorrÃ©lations
- Visualisations
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, avg, sum, min, max, stddev, corr
from pyspark.sql.types import DoubleType, IntegerType
import sys


def create_spark_session():
    """CrÃ©e et configure une session Spark"""
    spark = SparkSession.builder \
        .appName("Analyse Dataset Kaggle") \
        .config("spark.driver.memory", "2g") \
        .config("spark.sql.shuffle.partitions", "4") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    return spark


def load_data(spark, file_path):
    """Charge les donnÃ©es depuis un fichier CSV"""
    print(f"\n{'='*60}")
    print("CHARGEMENT DES DONNÃ‰ES")
    print(f"{'='*60}")
    
    df = spark.read.csv(file_path, header=True, inferSchema=True)
    print(f"âœ“ DonnÃ©es chargÃ©es avec succÃ¨s depuis: {file_path}")
    print(f"  Nombre de lignes: {df.count()}")
    print(f"  Nombre de colonnes: {len(df.columns)}")
    
    return df


def explore_data(df):
    """Explore les donnÃ©es et affiche les informations de base"""
    print(f"\n{'='*60}")
    print("EXPLORATION DES DONNÃ‰ES")
    print(f"{'='*60}")
    
    print("\nğŸ“Š Structure du dataset:")
    df.printSchema()
    
    print("\nğŸ“‹ AperÃ§u des donnÃ©es (5 premiÃ¨res lignes):")
    df.show(5, truncate=False)
    
    print("\nğŸ”¢ Statistiques descriptives:")
    df.describe().show()


def analyze_by_groups(df):
    """Analyse les donnÃ©es par groupes"""
    print(f"\n{'='*60}")
    print("ANALYSE PAR GROUPES")
    print(f"{'='*60}")
    
    # Analyse par dÃ©partement
    print("\nğŸ“ˆ Statistiques par dÃ©partement:")
    dept_stats = df.groupBy("department") \
        .agg(
            count("*").alias("nombre_employes"),
            avg("salary").alias("salaire_moyen"),
            avg("age").alias("age_moyen"),
            avg("experience_years").alias("experience_moyenne"),
            avg("satisfaction_score").alias("satisfaction_moyenne"),
            avg("performance_rating").alias("performance_moyenne")
        ) \
        .orderBy(col("salaire_moyen").desc())
    
    dept_stats.show(truncate=False)
    
    # Analyse par genre
    print("\nğŸ‘¥ Statistiques par genre:")
    gender_stats = df.groupBy("gender") \
        .agg(
            count("*").alias("nombre_employes"),
            avg("salary").alias("salaire_moyen"),
            avg("satisfaction_score").alias("satisfaction_moyenne"),
            avg("performance_rating").alias("performance_moyenne")
        )
    
    gender_stats.show(truncate=False)
    
    # Distribution par dÃ©partement
    print("\nğŸ“Š Distribution des employÃ©s par dÃ©partement:")
    dept_count = df.groupBy("department") \
        .agg(count("*").alias("nombre_employes")) \
        .orderBy(col("nombre_employes").desc())
    
    dept_count.show(truncate=False)


def calculate_correlations(df):
    """Calcule les corrÃ©lations entre variables numÃ©riques"""
    print(f"\n{'='*60}")
    print("ANALYSE DES CORRÃ‰LATIONS")
    print(f"{'='*60}")
    
    numeric_cols = ["age", "salary", "experience_years", "satisfaction_score", "performance_rating"]
    
    print("\nğŸ“ CorrÃ©lations entre variables numÃ©riques:")
    print("-" * 60)
    
    # CorrÃ©lation entre salaire et expÃ©rience
    corr_salary_exp = df.stat.corr("salary", "experience_years")
    print(f"CorrÃ©lation Salaire-ExpÃ©rience: {corr_salary_exp:.3f}")
    
    # CorrÃ©lation entre satisfaction et performance
    corr_sat_perf = df.stat.corr("satisfaction_score", "performance_rating")
    print(f"CorrÃ©lation Satisfaction-Performance: {corr_sat_perf:.3f}")
    
    # CorrÃ©lation entre Ã¢ge et expÃ©rience
    corr_age_exp = df.stat.corr("age", "experience_years")
    print(f"CorrÃ©lation Ã‚ge-ExpÃ©rience: {corr_age_exp:.3f}")
    
    # CorrÃ©lation entre salaire et performance
    corr_salary_perf = df.stat.corr("salary", "performance_rating")
    print(f"CorrÃ©lation Salaire-Performance: {corr_salary_perf:.3f}")


def advanced_analysis(df):
    """RÃ©alise des analyses avancÃ©es"""
    print(f"\n{'='*60}")
    print("ANALYSES AVANCÃ‰ES")
    print(f"{'='*60}")
    
    # EmployÃ©s Ã  haute performance
    print("\nâ­ EmployÃ©s Ã  haute performance (performance >= 4.5):")
    high_performers = df.filter(col("performance_rating") >= 4.5)
    print(f"Nombre: {high_performers.count()}")
    high_performers.select("id", "age", "department", "salary", "performance_rating") \
        .orderBy(col("performance_rating").desc()) \
        .show(5, truncate=False)
    
    # Analyse des salaires
    print("\nğŸ’° Analyse des salaires:")
    salary_ranges = df.groupBy("department") \
        .agg(
            min("salary").alias("salaire_min"),
            max("salary").alias("salaire_max"),
            avg("salary").alias("salaire_moyen")
        ) \
        .orderBy(col("salaire_moyen").desc())
    
    salary_ranges.show(truncate=False)
    
    # Taux de satisfaction par dÃ©partement
    print("\nğŸ˜Š Satisfaction moyenne par dÃ©partement:")
    satisfaction_by_dept = df.groupBy("department") \
        .agg(
            avg("satisfaction_score").alias("satisfaction_moyenne"),
            count("*").alias("nombre_employes")
        ) \
        .orderBy(col("satisfaction_moyenne").desc())
    
    satisfaction_by_dept.show(truncate=False)


def generate_summary(df):
    """GÃ©nÃ¨re un rÃ©sumÃ© de l'analyse"""
    print(f"\n{'='*60}")
    print("RÃ‰SUMÃ‰ DE L'ANALYSE")
    print(f"{'='*60}")
    
    total_employees = df.count()
    avg_salary = df.agg(avg("salary")).collect()[0][0]
    avg_age = df.agg(avg("age")).collect()[0][0]
    avg_satisfaction = df.agg(avg("satisfaction_score")).collect()[0][0]
    avg_performance = df.agg(avg("performance_rating")).collect()[0][0]
    
    print(f"""
ğŸ“Š Vue d'ensemble du dataset:
   - Nombre total d'employÃ©s: {total_employees}
   - Salaire moyen: ${avg_salary:,.2f}
   - Ã‚ge moyen: {avg_age:.1f} ans
   - Score de satisfaction moyen: {avg_satisfaction:.2f}/10
   - Note de performance moyenne: {avg_performance:.2f}/5
   
ğŸ¢ DÃ©partements: {df.select("department").distinct().count()}
ğŸ‘¥ Distribution par genre: {df.groupBy("gender").count().count()} catÃ©gories
    """)
    
    # Insights clÃ©s
    print("ğŸ’¡ Insights clÃ©s:")
    
    # DÃ©partement avec le meilleur salaire
    best_paid_dept = df.groupBy("department") \
        .agg(avg("salary").alias("avg_sal")) \
        .orderBy(col("avg_sal").desc()) \
        .first()
    print(f"   - DÃ©partement le mieux payÃ©: {best_paid_dept['department']} (${best_paid_dept['avg_sal']:,.2f})")
    
    # DÃ©partement avec la meilleure satisfaction
    best_satisfaction_dept = df.groupBy("department") \
        .agg(avg("satisfaction_score").alias("avg_sat")) \
        .orderBy(col("avg_sat").desc()) \
        .first()
    print(f"   - DÃ©partement avec la meilleure satisfaction: {best_satisfaction_dept['department']} ({best_satisfaction_dept['avg_sat']:.2f}/10)")


def main():
    """Fonction principale"""
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                            â•‘
    â•‘         ANALYSE PYSPARK - DATASET KAGGLE                   â•‘
    â•‘                                                            â•‘
    â•‘    Ã‰tude complÃ¨te d'un dataset d'employÃ©s                  â•‘
    â•‘                                                            â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # CrÃ©er la session Spark
    spark = create_spark_session()
    
    try:
        # Chemin du fichier de donnÃ©es
        data_path = "data/employee_data.csv"
        
        # Charger les donnÃ©es
        df = load_data(spark, data_path)
        
        # Explorer les donnÃ©es
        explore_data(df)
        
        # Analyser par groupes
        analyze_by_groups(df)
        
        # Calculer les corrÃ©lations
        calculate_correlations(df)
        
        # Analyses avancÃ©es
        advanced_analysis(df)
        
        # GÃ©nÃ©rer le rÃ©sumÃ©
        generate_summary(df)
        
        print(f"\n{'='*60}")
        print("âœ“ ANALYSE TERMINÃ‰E AVEC SUCCÃˆS")
        print(f"{'='*60}\n")
        
    except Exception as e:
        print(f"\nâŒ Erreur lors de l'analyse: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    finally:
        # ArrÃªter la session Spark
        spark.stop()


if __name__ == "__main__":
    main()
