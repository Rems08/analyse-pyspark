# Analyse PySpark - Dataset Kaggle

![PySpark](https://img.shields.io/badge/PySpark-3.5+-orange.svg)
![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-1.40+-red.svg)

## ğŸ“– Description

Ce projet rÃ©alise une analyse complÃ¨te d'un dataset en utilisant **Apache PySpark**. Il dÃ©montre l'utilisation de PySpark pour l'exploration de donnÃ©es, les statistiques descriptives, les analyses par groupes, et les corrÃ©lations.

**Nouveau** : Une interface interactive **Streamlit** permet de visualiser et d'explorer les donnÃ©es de maniÃ¨re conviviale.

## ğŸ¯ Objectifs

- Charger et explorer un dataset type Kaggle
- RÃ©aliser des statistiques descriptives avec PySpark
- Analyser les donnÃ©es par groupes (dÃ©partements, genre)
- Calculer des corrÃ©lations entre variables
- GÃ©nÃ©rer des insights business
- **Visualiser les donnÃ©es avec une interface Streamlit interactive**

## ğŸ“Š Dataset

Le dataset utilisÃ© contient des donnÃ©es d'employÃ©s avec les informations suivantes:
- **id**: Identifiant unique de l'employÃ©
- **age**: Ã‚ge de l'employÃ©
- **gender**: Genre (M/F)
- **salary**: Salaire annuel
- **department**: DÃ©partement (IT, HR, Sales, Marketing)
- **experience_years**: AnnÃ©es d'expÃ©rience
- **satisfaction_score**: Score de satisfaction (0-10)
- **performance_rating**: Note de performance (0-5)

## ğŸš€ Installation

### PrÃ©requis

- Python 3.8 ou supÃ©rieur
- Java 17 (requis pour PySpark)
- [Mise](https://mise.jdx.dev/) (gestionnaire de tÃ¢ches)
- Apache Spark (optionnel, pour le mode cluster)

### Installation des dÃ©pendances

```bash
# Avec mise (recommandÃ©)
mise run install

# Ou avec pip
pip install -r requirements.txt
```

### Installation de Spark (optionnel)

Pour utiliser le mode cluster avec Master/Workers :

```bash
# macOS avec Homebrew
brew install openjdk@17
brew install apache-spark

# Voir les instructions complÃ¨tes
mise run install-spark
```

## ğŸ’» Utilisation

### ğŸŒ Interface Streamlit (recommandÃ©)

Lancez l'application Streamlit pour une exploration interactive des donnÃ©es :

```bash
# Avec mise
mise run streamlit

# Ou directement
streamlit run streamlit_app.py
```

L'interface Streamlit offre :
- **ğŸ“ˆ Vue d'ensemble** : MÃ©triques clÃ©s et graphiques de synthÃ¨se
- **ğŸ¢ Analyse par DÃ©partement** : Statistiques dÃ©taillÃ©es par dÃ©partement
- **ğŸ“Š Distributions** : Histogrammes et scatter plots interactifs
- **ğŸ”— CorrÃ©lations** : Matrice de corrÃ©lation et heatmap
- **ğŸ“‹ DonnÃ©es brutes** : Exploration et export des donnÃ©es

**Filtres disponibles** :
- DÃ©partement
- Genre
- Tranche d'Ã¢ge
- Tranche de salaire

### Mode simple (local)

```bash
# Avec mise
mise run analyse

# Ou directement
python analyse.py
```

### Mode interactif

```bash
# Shell PySpark
mise run pyspark-shell

# Jupyter Notebook
mise run notebook
```

## ğŸ¬ DÃ©mo : Cluster Spark avec Master et Workers

Cette section dÃ©crit la procÃ©dure complÃ¨te pour dÃ©marrer un cluster Spark local avec un Master et plusieurs Workers, idÃ©al pour une dÃ©monstration du traitement distribuÃ©.

### Ã‰tape 1 : VÃ©rifier les prÃ©requis

```bash
# VÃ©rifier que Java est installÃ©
java -version

# VÃ©rifier que Spark est installÃ©
spark-shell --version

# Afficher les infos du projet
mise run info
```

### Ã‰tape 2 : DÃ©marrer le cluster Spark

**Option A : DÃ©marrage automatique (recommandÃ©)**

```bash
# DÃ©marrer le cluster complet (1 Master + 2 Workers)
mise run spark-cluster-start
```

**Option B : DÃ©marrage manuel Ã©tape par Ã©tape**

```bash
# Terminal 1 : DÃ©marrer le Master
mise run spark-master-start

# Terminal 2 : DÃ©marrer le Worker 1
mise run spark-worker-start

# Terminal 3 : DÃ©marrer un second Worker (optionnel)
mise run spark-worker-start
```

### Ã‰tape 3 : VÃ©rifier le statut du cluster

```bash
# Afficher le statut
mise run spark-cluster-status
```

**Interfaces Web disponibles :**

| Composant | URL |
|-----------|-----|
| ğŸ–¥ï¸ Master UI | http://localhost:8080 |
| ğŸ‘· Worker 1 UI | http://localhost:8081 |
| ğŸ‘· Worker 2 UI | http://localhost:8082 |

### Ã‰tape 4 : ExÃ©cuter l'analyse sur le cluster

```bash
# Soumettre le job au cluster
mise run analyse-cluster
```

Ou utiliser le shell PySpark connectÃ© au cluster :

```bash
mise run pyspark-shell-cluster
```

### Ã‰tape 5 : Observer l'exÃ©cution

1. Ouvrez http://localhost:8080 dans votre navigateur
2. Observez les **Workers** enregistrÃ©s
3. Cliquez sur **Running Applications** pour voir le job en cours
4. Explorez les **Executors** et les **Stages**

### Ã‰tape 6 : ArrÃªter le cluster

```bash
# ArrÃªter tout le cluster
mise run spark-cluster-stop
```

### ğŸ“‹ RÃ©sumÃ© des commandes Mise pour le cluster

| Commande | Description |
|----------|-------------|
| `mise run spark-cluster-start` | ğŸŒŸ DÃ©marrer Master + 2 Workers |
| `mise run spark-cluster-stop` | ğŸ›‘ ArrÃªter tout le cluster |
| `mise run spark-cluster-status` | ğŸ“Š Voir le statut du cluster |
| `mise run spark-master-start` | ğŸš€ DÃ©marrer uniquement le Master |
| `mise run spark-master-stop` | ğŸ›‘ ArrÃªter le Master |
| `mise run spark-worker-start` | ğŸ‘· DÃ©marrer un Worker |
| `mise run spark-worker-stop` | ğŸ›‘ ArrÃªter les Workers |
| `mise run analyse-cluster` | ğŸ“Š ExÃ©cuter l'analyse sur le cluster |
| `mise run pyspark-shell-cluster` | ğŸ Shell connectÃ© au cluster |

### ğŸ’¡ Tips pour la dÃ©mo

1. **Ouvrez le Master UI** avant de lancer le job pour voir les Workers s'enregistrer
2. **Utilisez plusieurs terminaux** pour montrer le dÃ©marrage sÃ©quentiel
3. **Montrez les logs** dans la console du Worker pendant l'exÃ©cution
4. **Comparez les performances** entre mode local et mode cluster

## ğŸ“ˆ Analyses rÃ©alisÃ©es

Le script effectue les analyses suivantes:

1. **Exploration des donnÃ©es**
   - Structure du dataset
   - AperÃ§u des premiÃ¨res lignes
   - Statistiques descriptives

2. **Analyse par groupes**
   - Statistiques par dÃ©partement
   - Statistiques par genre
   - Distribution des employÃ©s

3. **CorrÃ©lations**
   - Salaire vs ExpÃ©rience
   - Satisfaction vs Performance
   - Ã‚ge vs ExpÃ©rience
   - Salaire vs Performance

4. **Analyses avancÃ©es**
   - EmployÃ©s Ã  haute performance
   - Analyse des fourchettes de salaires
   - Taux de satisfaction par dÃ©partement

5. **RÃ©sumÃ© et insights clÃ©s**
   - Vue d'ensemble du dataset
   - Insights business

## ğŸ“ Structure du projet

```
analyse-pyspark/
â”œâ”€â”€ analyse.py              # Script principal d'analyse
â”œâ”€â”€ data/
â”‚   â””â”€â”€ employee_data.csv   # Dataset d'exemple
â”œâ”€â”€ requirements.txt        # DÃ©pendances Python
â”œâ”€â”€ README.md              # Documentation
â””â”€â”€ .gitignore             # Fichiers Ã  ignorer
```

## ğŸ” Exemple de sortie

Le script gÃ©nÃ¨re une sortie complÃ¨te avec:
- Statistiques descriptives des variables numÃ©riques
- Tableaux d'agrÃ©gation par dÃ©partement et genre
- CorrÃ©lations entre variables
- Insights clÃ©s sur les donnÃ©es

## ğŸ“ MÃ©thodologie

L'analyse utilise les capacitÃ©s distribuÃ©es de PySpark pour:
- Traiter efficacement de grands volumes de donnÃ©es
- Effectuer des agrÃ©gations et groupements
- Calculer des statistiques en parallÃ¨le
- Optimiser les performances avec le lazy evaluation

## ğŸ› ï¸ Technologies utilisÃ©es

- **PySpark**: Framework de traitement distribuÃ©
- **Python**: Langage de programmation
- **Pandas**: Manipulation de donnÃ©es (optionnel)
- **Matplotlib/Seaborn**: Visualisations (optionnel)

## ğŸ“š Ressources

- [Documentation PySpark](https://spark.apache.org/docs/latest/api/python/)
- [Apache Spark](https://spark.apache.org/)
- [Kaggle Datasets](https://www.kaggle.com/datasets)

## ğŸ‘¤ Auteur

Rems08

## ğŸ“„ Licence

Ce projet est sous licence MIT - voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.